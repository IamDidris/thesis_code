{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# ==============================\n",
        "# 0. Install\n",
        "# ==============================\n",
        "!pip install sentence-transformers rouge-score scikit-learn\n",
        "!pip install openpyxl\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nsIJp8wQKz5L",
        "outputId": "4e7077f8-d737-4a02-ac74-61d002181988"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.11/dist-packages (3.4.1)\n",
            "Requirement already satisfied: rouge-score in /usr/local/lib/python3.11/dist-packages (0.1.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (4.51.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (4.67.1)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (2.6.0+cu124)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (1.14.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (0.30.2)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (11.1.0)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.11/dist-packages (from rouge-score) (1.4.0)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (from rouge-score) (3.9.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from rouge-score) (2.0.2)\n",
            "Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.11/dist-packages (from rouge-score) (1.17.0)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.18.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2025.3.2)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2.32.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (4.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.5.3)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk->rouge-score) (8.1.8)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2025.1.31)\n",
            "Requirement already satisfied: openpyxl in /usr/local/lib/python3.11/dist-packages (3.1.5)\n",
            "Requirement already satisfied: et-xmlfile in /usr/local/lib/python3.11/dist-packages (from openpyxl) (2.0.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================\n",
        "# 1. Imports\n",
        "# ==============================\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "from rouge_score import rouge_scorer\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "sbert_model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n",
        "\n",
        "rouge_scorer_instance = rouge_scorer.RougeScorer(['rougeL'], use_stemmer=True)"
      ],
      "metadata": {
        "id": "fuXExMaVPeUV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================\n",
        "# 2. Preprocess\n",
        "# ==============================\n",
        "def preprocess(text):\n",
        "    return text.strip()\n",
        "\n",
        "# ==============================\n",
        "# 3. SBERT Similarity (3 texts)\n",
        "# ==============================\n",
        "def compute_sbert_similarity(text1, text2, text3):\n",
        "    emb1 = sbert_model.encode(preprocess(text1), convert_to_tensor=True)\n",
        "    emb2 = sbert_model.encode(preprocess(text2), convert_to_tensor=True)\n",
        "    emb3 = sbert_model.encode(preprocess(text3), convert_to_tensor=True)\n",
        "\n",
        "    sim11 = util.cos_sim(emb1, emb1).item()\n",
        "    sim12 = util.cos_sim(emb1, emb2).item()\n",
        "    sim13 = util.cos_sim(emb1, emb3).item()\n",
        "    sim21 = util.cos_sim(emb2, emb1).item()\n",
        "    sim22 = util.cos_sim(emb2, emb2).item()\n",
        "    sim23 = util.cos_sim(emb2, emb3).item()\n",
        "    sim31 = util.cos_sim(emb3, emb1).item()\n",
        "    sim32 = util.cos_sim(emb3, emb2).item()\n",
        "    sim33 = util.cos_sim(emb3, emb3).item()\n",
        "\n",
        "    return {\n",
        "        's11': sim11, 's12': sim12, 's13': sim13,\n",
        "        's21': sim21, 's22': sim22, 's23': sim23,\n",
        "        's31': sim31, 's32': sim32, 's33': sim33\n",
        "    }\n",
        "\n",
        "# ==============================\n",
        "# 4. ROUGE-L Similarity (3 texts)\n",
        "# ==============================\n",
        "def compute_rouge_similarity(text1, text2, text3):\n",
        "    r11 = rouge_scorer_instance.score(preprocess(text1), preprocess(text1))['rougeL'].fmeasure\n",
        "    r12 = rouge_scorer_instance.score(preprocess(text1), preprocess(text2))['rougeL'].fmeasure\n",
        "    r13 = rouge_scorer_instance.score(preprocess(text1), preprocess(text3))['rougeL'].fmeasure\n",
        "    r21 = rouge_scorer_instance.score(preprocess(text2), preprocess(text1))['rougeL'].fmeasure\n",
        "    r22 = rouge_scorer_instance.score(preprocess(text2), preprocess(text2))['rougeL'].fmeasure\n",
        "    r23 = rouge_scorer_instance.score(preprocess(text2), preprocess(text3))['rougeL'].fmeasure\n",
        "    r31 = rouge_scorer_instance.score(preprocess(text3), preprocess(text1))['rougeL'].fmeasure\n",
        "    r32 = rouge_scorer_instance.score(preprocess(text3), preprocess(text2))['rougeL'].fmeasure\n",
        "    r33 = rouge_scorer_instance.score(preprocess(text3), preprocess(text3))['rougeL'].fmeasure\n",
        "\n",
        "    return {\n",
        "        'r11': r11, 'r12': r12, 'r13': r13,\n",
        "        'r21': r21, 'r22': r22, 'r23': r23,\n",
        "        'r31': r31, 'r32': r32, 'r33': r33\n",
        "    }\n",
        "\n",
        "# ==============================\n",
        "# 5. Combined Similarity (3 texts)\n",
        "# ==============================\n",
        "def compute_combined_score(text1, text2, text3):\n",
        "\n",
        "    sbert_dict = compute_sbert_similarity(text1, text2, text3)\n",
        "    rouge_dict = compute_rouge_similarity(text1, text2, text3)\n",
        "\n",
        "    c11 = 0.85*sbert_dict['s11'] + 0.15*rouge_dict['r11']\n",
        "    c12 = 0.85*sbert_dict['s12'] + 0.15*rouge_dict['r12']\n",
        "    c13 = 0.85*sbert_dict['s13'] + 0.15*rouge_dict['r13']\n",
        "    c21 = 0.85*sbert_dict['s21'] + 0.15*rouge_dict['r21']\n",
        "    c22 = 0.85*sbert_dict['s22'] + 0.15*rouge_dict['r22']\n",
        "    c23 = 0.85*sbert_dict['s23'] + 0.15*rouge_dict['r23']\n",
        "    c31 = 0.85*sbert_dict['s31'] + 0.15*rouge_dict['r31']\n",
        "    c32 = 0.85*sbert_dict['s32'] + 0.15*rouge_dict['r32']\n",
        "    c33 = 0.85*sbert_dict['s33'] + 0.15*rouge_dict['r33']\n",
        "\n",
        "    return {\n",
        "        # SBERT\n",
        "        's11': sbert_dict['s11'], 's12': sbert_dict['s12'], 's13': sbert_dict['s13'],\n",
        "        's21': sbert_dict['s21'], 's22': sbert_dict['s22'], 's23': sbert_dict['s23'],\n",
        "        's31': sbert_dict['s31'], 's32': sbert_dict['s32'], 's33': sbert_dict['s33'],\n",
        "\n",
        "        # ROUGE\n",
        "        'r11': rouge_dict['r11'], 'r12': rouge_dict['r12'], 'r13': rouge_dict['r13'],\n",
        "        'r21': rouge_dict['r21'], 'r22': rouge_dict['r22'], 'r23': rouge_dict['r23'],\n",
        "        'r31': rouge_dict['r31'], 'r32': rouge_dict['r32'], 'r33': rouge_dict['r33'],\n",
        "\n",
        "        # Combined\n",
        "        'c11': c11, 'c12': c12, 'c13': c13,\n",
        "        'c21': c21, 'c22': c22, 'c23': c23,\n",
        "        'c31': c31, 'c32': c32, 'c33': c33\n",
        "    }\n",
        "\n",
        "# ==============================\n",
        "# 6. Main Evaluation Function\n",
        "# ==============================\n",
        "def evaluate_description_pairs(text1, text2, text3):\n",
        "    \"\"\"\n",
        "    Computes SBERT, ROUGE-L, and Combined for all\n",
        "    (i->j) pairs of the 3 texts.\n",
        "    Returns three DataFrames (3×3 each): SBERT, ROUGE, and Combined\n",
        "    \"\"\"\n",
        "    d = compute_combined_score(text1, text2, text3)\n",
        "\n",
        "    sbert_data = []\n",
        "    rouge_data = []\n",
        "    combined_data = []\n",
        "\n",
        "    for i in range(3):\n",
        "        s_row = []\n",
        "        r_row = []\n",
        "        c_row = []\n",
        "        for j in range(3):\n",
        "            s_row.append(round(d[f's{i+1}{j+1}'], 3))\n",
        "            r_row.append(round(d[f'r{i+1}{j+1}'], 3))\n",
        "            c_row.append(round(d[f'c{i+1}{j+1}'], 3))\n",
        "        sbert_data.append(s_row)\n",
        "        rouge_data.append(r_row)\n",
        "        combined_data.append(c_row)\n",
        "\n",
        "    index_labels = [f\"Text{i+1}\" for i in range(3)]\n",
        "    columns_labels = [f\"Text{j+1}\" for j in range(3)]\n",
        "\n",
        "    df_sbert = pd.DataFrame(sbert_data, index=index_labels, columns=columns_labels)\n",
        "    df_rouge = pd.DataFrame(rouge_data, index=index_labels, columns=columns_labels)\n",
        "    df_combined = pd.DataFrame(combined_data, index=index_labels, columns=columns_labels)\n",
        "\n",
        "    return df_sbert, df_rouge, df_combined\n",
        "# ==============================\n",
        "# 7. Example Usage\n",
        "# ==============================\n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "    txt1 = \"\"\"The associated files porvide an implementation of the trie data structure. The structure provides fast  from strings to values. The code features functionaly for creating and destroying tries, inserting and  removing values from the trie, looking up values and finding the number of entries.  trie.c implements these functions by allocating trie nodes as needed along each path of characters in  the key, incrementing a use_count so the code knows when a node is no longer referenced and can be freed.  This mechanism makes insertions, lookups, and removals work in a straightforward way, each character in the  key selects the correct branch of the tree, with ample space in each node’s next array for all possible byte  values (0–255). The trie keeps track of values (void pointers) directly within each node,  along with a sentinel constant (TRIE_NULL) to mark unused entries.\"\"\"\n",
        "    txt2 = \"\"\"The provided code implements a trie (prefix tree) data structure in C, which allows fast mapping of strings (or binary keys) to associated values. The trie supports insertion, lookup, and deletion of key-value pairs efficiently.  The implementation defines a Trie structure that holds a root node, and each node (TrieNode) maintains a value, a usage count, and an array of 256 pointers to child nodes, representing all possible byte values.  Key Functionalities: Creation & Deletion:  trie_new() initializes a new trie. trie_free() recursively deallocates all nodes. Insertion:  trie_insert() inserts a string key and its associated value. trie_insert_binary() does the same for binary keys. If a key already exists, its value is updated. Lookup:  trie_lookup() retrieves the value associated with a string key. trie_lookup_binary() does the same for binary keys. Deletion:  trie_remove() removes a key and reclaims memory if no other keys share its nodes. trie_remove_binary() is its counterpart for binary keys. Utility:  trie_num_entries() returns the total number of stored entries. The code ensures memory efficiency by freeing unused nodes when keys are deleted. It also includes rollback mechanisms to handle failed memory allocations during insertion. This trie implementation is useful for applications requiring fast prefix-based lookups, such as dictionaries, auto-completion, or routing tables.\"\"\"\n",
        "    txt3 = \"\"\"This C code implements a trie (prefix tree) data structure, providing a flexible way to store and retrieve key-value pairs where keys are strings or binary sequences. The core of the implementation revolves around dynamically allocated nodes, each representing a character in the key and capable of holding a value (TrieValue). Each node can branch into 256 directions (one for each possible byte value), enabling fast lookup, insertion, and deletion.  The code defines functions to create (trie_new) and free (trie_free) the trie. Insertion functions (trie_insert and trie_insert_binary) add keys and associated values to the trie, either as null-terminated strings or binary data. If memory allocation fails during insertion, a rollback mechanism undoes any partial additions. Lookup functions retrieve stored values (trie_lookup, trie_lookup_binary), while removal functions (trie_remove, trie_remove_binary) delete entries and clean up unused nodes based on reference counting (use_count).  The design ensures memory efficiency and safety by recursively freeing nodes and tracking usage to avoid memory leaks. It supports general-purpose usage by treating values as opaque pointers (void *), allowing flexibility in the kind of data stored. Overall, the code offers a robust, low-level trie implementation suitable for fast associative data storage with both textual and binary keys.\"\"\"\n",
        "\n",
        "    # Compute the matrices\n",
        "    df_sbert, df_rouge, df_combined = evaluate_description_pairs(txt1, txt2, txt3)\n",
        "\n",
        "    # Print each matrix\n",
        "    print(\"=== SBERT Similarity Matrix ===\")\n",
        "    print(df_sbert)\n",
        "\n",
        "    print(\"\\n=== ROUGE-L Similarity Matrix ===\")\n",
        "    print(df_rouge)\n",
        "\n",
        "    print(\"\\n=== Combined Similarity Matrix ===\")\n",
        "    print(df_combined)\n",
        "\n",
        "    with pd.ExcelWriter(\"trie.xlsx\", engine='openpyxl') as writer:\n",
        "      df_sbert.to_excel(writer, sheet_name='SBERT', index=True)\n",
        "      df_rouge.to_excel(writer, sheet_name='ROUGE-L', index=True)\n",
        "      df_combined.to_excel(writer, sheet_name='Combined', index=True)\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "-2_iFtgvKxAq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9c68532e-40be-4560-db5f-4013699ad8fe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== SBERT Similarity Matrix ===\n",
            "       Text1  Text2  Text3\n",
            "Text1  1.000  0.835  0.862\n",
            "Text2  0.835  1.000  0.918\n",
            "Text3  0.862  0.918  1.000\n",
            "\n",
            "=== ROUGE-L Similarity Matrix ===\n",
            "       Text1  Text2  Text3\n",
            "Text1  1.000  0.199  0.184\n",
            "Text2  0.199  1.000  0.307\n",
            "Text3  0.184  0.307  1.000\n",
            "\n",
            "=== Combined Similarity Matrix ===\n",
            "       Text1  Text2  Text3\n",
            "Text1   1.00  0.740  0.760\n",
            "Text2   0.74  1.000  0.826\n",
            "Text3   0.76  0.826  1.000\n",
            "\n",
            "All similarity matrices saved to all_similarities.xlsx with separate sheets.\n"
          ]
        }
      ]
    }
  ]
}